{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6175060f-397f-49a9-9024-935956fc0a6c",
   "metadata": {},
   "source": [
    "## Setting up to Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc20c5-73de-4fd1-a0c3-e8dcf96597af",
   "metadata": {},
   "source": [
    "The first part of the process, importing the libraries and depend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538b360e-c563-4015-9098-e0bc317112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3b688-a325-475d-b92c-b1c3b199edab",
   "metadata": {},
   "source": [
    "Loading dataset, after downloading it from kaggle: https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9b17e0-25b2-4cda-b3f2-4d431c909962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes_prediction_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6746e-eb6f-4067-90b9-db8d7b9cc397",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c868351-8a3d-4d36-8540-6f0db7fac962",
   "metadata": {},
   "source": [
    "The first steps are to understand the data, in order to do so, first we may get some general information about the dataset through the `head()`, `info()` and `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38abd20b-56ba-48c2-90ab-3274ef629a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>smoking_history</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42197</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57911</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.59</td>\n",
       "      <td>7.0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76268</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>22.46</td>\n",
       "      <td>6.5</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79982</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>40.33</td>\n",
       "      <td>8.2</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82655</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>31.41</td>\n",
       "      <td>6.2</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
       "42197   Male  23.0             0              0         No Info  27.32   \n",
       "57911   Male  23.0             0              0         No Info  27.59   \n",
       "76268   Male  23.0             0              0           never  22.46   \n",
       "79982   Male  23.0             0              0         No Info  40.33   \n",
       "82655   Male  23.0             0              0           never  31.41   \n",
       "\n",
       "       HbA1c_level  blood_glucose_level  diabetes  \n",
       "42197          5.7                  145         1  \n",
       "57911          7.0                  159         1  \n",
       "76268          6.5                  140         1  \n",
       "79982          8.2                  220         1  \n",
       "82655          6.2                  140         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['gender'] == 'Male') & (df['age'] == 23.0) & (df['diabetes'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13223a1-8174-4d70-957f-42db0229d841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>smoking_history</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>never</td>\n",
       "      <td>25.19</td>\n",
       "      <td>6.6</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.32</td>\n",
       "      <td>6.6</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>current</td>\n",
       "      <td>23.45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>current</td>\n",
       "      <td>20.14</td>\n",
       "      <td>4.8</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
       "0  Female  80.0             0              1           never  25.19   \n",
       "1  Female  54.0             0              0         No Info  27.32   \n",
       "2    Male  28.0             0              0           never  27.32   \n",
       "3  Female  36.0             0              0         current  23.45   \n",
       "4    Male  76.0             1              1         current  20.14   \n",
       "\n",
       "   HbA1c_level  blood_glucose_level  diabetes  \n",
       "0          6.6                  140         0  \n",
       "1          6.6                   80         0  \n",
       "2          5.7                  158         0  \n",
       "3          5.0                  155         0  \n",
       "4          4.8                  155         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abee033c-4469-41c0-9b9a-d904f719f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   gender               100000 non-null  object \n",
      " 1   age                  100000 non-null  float64\n",
      " 2   hypertension         100000 non-null  int64  \n",
      " 3   heart_disease        100000 non-null  int64  \n",
      " 4   smoking_history      100000 non-null  object \n",
      " 5   bmi                  100000 non-null  float64\n",
      " 6   HbA1c_level          100000 non-null  float64\n",
      " 7   blood_glucose_level  100000 non-null  int64  \n",
      " 8   diabetes             100000 non-null  int64  \n",
      "dtypes: float64(3), int64(4), object(2)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c05d5f0-e371-4ddb-8b30-aee1bb320204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.885856</td>\n",
       "      <td>0.07485</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>27.320767</td>\n",
       "      <td>5.527507</td>\n",
       "      <td>138.058060</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.516840</td>\n",
       "      <td>0.26315</td>\n",
       "      <td>0.194593</td>\n",
       "      <td>6.636783</td>\n",
       "      <td>1.070672</td>\n",
       "      <td>40.708136</td>\n",
       "      <td>0.278883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.010000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.630000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.320000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.690000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  hypertension  heart_disease            bmi  \\\n",
       "count  100000.000000  100000.00000  100000.000000  100000.000000   \n",
       "mean       41.885856       0.07485       0.039420      27.320767   \n",
       "std        22.516840       0.26315       0.194593       6.636783   \n",
       "min         0.080000       0.00000       0.000000      10.010000   \n",
       "25%        24.000000       0.00000       0.000000      23.630000   \n",
       "50%        43.000000       0.00000       0.000000      27.320000   \n",
       "75%        60.000000       0.00000       0.000000      29.580000   \n",
       "max        80.000000       1.00000       1.000000      95.690000   \n",
       "\n",
       "         HbA1c_level  blood_glucose_level       diabetes  \n",
       "count  100000.000000        100000.000000  100000.000000  \n",
       "mean        5.527507           138.058060       0.085000  \n",
       "std         1.070672            40.708136       0.278883  \n",
       "min         3.500000            80.000000       0.000000  \n",
       "25%         4.800000           100.000000       0.000000  \n",
       "50%         5.800000           140.000000       0.000000  \n",
       "75%         6.200000           159.000000       0.000000  \n",
       "max         9.000000           300.000000       1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06bb65-5129-4e80-8939-97b875983abd",
   "metadata": {},
   "source": [
    "So, after a brief analysis we may conclude:\n",
    "- There are a plentiful number of data available, 100.000 cases.\n",
    "- The data doesn't contain any explicit error-leading missing values such as NAs, but it has the `No info` class of `smoking_history`.\n",
    "- There are two categorical variables, `gender` and `smoking_history`, both will have to be transformed to a numerical value.\n",
    "- `diabetes`, the target, is a boolean value and have a mean of 0.085. Meaning that only 8.5% of the cases in fact have diabetes implying on a imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1582abb-8484-48a9-b304-63a1644bf571",
   "metadata": {},
   "source": [
    "Next, we going to analyse the `smoking_history` feature as it appears to be problematic for being a categorical feature with missing values. \n",
    "\n",
    "The first step is to check the possible values this feature can take and their respective frequencies. As shown in the next cell, this feature has a couple of issues:\n",
    "- `No Info` appears in 35916 cases, meaning that more than one-third of the cases has a unespecified value on this feature (missing data).\n",
    "- There are ambiguous and overlapping categories. For example, `not current` could mean the same as `former` or `never`, and the criteria that distinguise `ever` from `current` is poorly defined.\n",
    "\n",
    "Considering the high number of unkown values and the ambiguity in class definition, a further evaluations is needed to assess this feature impact on the target prediction. This will help justify the efford of keeping this feature, or determine if it should be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9d3902-750c-45df-9779-876ad189901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smoking_history\n",
       "No Info        35816\n",
       "never          35095\n",
       "former          9352\n",
       "current         9286\n",
       "not current     6447\n",
       "ever            4004\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['smoking_history'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac6d59-a7bb-41e2-896e-8e5cd6df8018",
   "metadata": {},
   "source": [
    "In order to further evaluate, we'll define a `ColumnTransformer` and transform the categorical values through the One-Hot Encoding. With the categorical data transformed, we can then use some feature metrics, such as Mutual Information score and correlation, to understand the features relevance to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a609ce8-983e-4a87-91da-ca21434e06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separetes the Features from the Target.\n",
    "X = df.drop(['diabetes'], axis=1)\n",
    "y = df['diabetes']\n",
    "\n",
    "CT = ColumnTransformer(\n",
    "    transformers = [ \n",
    "        ('onehot', OneHotEncoder(sparse_output=False, categories='auto'), ['gender', 'smoking_history']), #sparse_output=False\n",
    "        #('ordinal', OrdinalEncoder(categories=[['never','No Info', 'not current', 'former', 'current', 'ever']]), ['smoking_history']) #sparse_output=False\n",
    "    ],\t\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8805fe42-bb18-48ef-aa12-ae9b6bf8cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the OneHot onto categorical features.\n",
    "X_encoded = CT.fit_transform(X[['gender', 'smoking_history']])\n",
    "encoded_cols = CT.get_feature_names_out(['gender', 'smoking_history'])\n",
    "\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_cols, index=X.index)\n",
    "X_features = pd.concat([X.drop(['gender', 'smoking_history'], axis=1), X_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee96c9e-c566-4389-ab75-f04abad7d540",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "Analyzing the correlation between the features and the target variable, allows us to see which of the features are most strong linear related to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605874fa-716a-4796-961e-6e79eeed724b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diabetes                               1.000000\n",
       "blood_glucose_level                    0.419558\n",
       "HbA1c_level                            0.400660\n",
       "age                                    0.258008\n",
       "bmi                                    0.214357\n",
       "hypertension                           0.197823\n",
       "heart_disease                          0.171727\n",
       "onehot__smoking_history_former         0.097917\n",
       "onehot__gender_Male                    0.037666\n",
       "onehot__smoking_history_never          0.027267\n",
       "onehot__smoking_history_ever           0.024080\n",
       "onehot__smoking_history_not current    0.020734\n",
       "onehot__smoking_history_current        0.019606\n",
       "onehot__gender_Other                  -0.004090\n",
       "onehot__gender_Female                 -0.037553\n",
       "onehot__smoking_history_No Info       -0.118939\n",
       "Name: diabetes, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans = pd.concat([X_features, y], axis=1)\n",
    "df_trans.corr()['diabetes'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c649e1-3c5d-4d5e-9976-3ee051b54584",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "Mutual Information can capture many types of relationships that each variable may have with the target, not being limited by linear associations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f5c4fa-6374-4697-a547-16abc5d127b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Feature  MI Score\n",
      "4                           HbA1c_level  0.130564\n",
      "5                   blood_glucose_level  0.114293\n",
      "0                                   age  0.039955\n",
      "3                                   bmi  0.027423\n",
      "6                 onehot__gender_Female  0.017034\n",
      "9       onehot__smoking_history_No Info  0.015438\n",
      "1                          hypertension  0.012685\n",
      "7                   onehot__gender_Male  0.009468\n",
      "2                         heart_disease  0.009383\n",
      "13        onehot__smoking_history_never  0.007803\n",
      "12       onehot__smoking_history_former  0.005175\n",
      "11         onehot__smoking_history_ever  0.002307\n",
      "10      onehot__smoking_history_current  0.000733\n",
      "8                  onehot__gender_Other  0.000000\n",
      "14  onehot__smoking_history_not current  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Calcula a MI\n",
    "mi_scores = mutual_info_classif(X_features, y)\n",
    "\n",
    "# Exibe os resultados\n",
    "mi_df = pd.DataFrame({'Feature': X_features.columns, 'MI Score': mi_scores})\n",
    "print(mi_df.sort_values(by='MI Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902358-acad-4d6a-8c50-8ab99f582478",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Using PCA we may obtain valueble information about components made using the linear combination of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d483e3b-6bed-49d6-b650-d731bb891e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PC1       PC2       PC3       PC4       PC5\n",
      "age                  0.566264 -0.265468 -0.057950 -0.156845 -0.023003\n",
      "hypertension         0.421370 -0.175895  0.029841  0.864849  0.076444\n",
      "heart_disease        0.352297 -0.143349  0.818735 -0.296400 -0.018912\n",
      "bmi                  0.456550 -0.218234 -0.569784 -0.360819 -0.056236\n",
      "HbA1c_level          0.284349  0.656053  0.000659  0.058274 -0.696617\n",
      "blood_glucose_level  0.297298  0.632461 -0.027746 -0.077440  0.710515\n"
     ]
    }
   ],
   "source": [
    "X_filtered = X_features[[\"age\", \"hypertension\", \"heart_disease\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]]\n",
    "X_scaled = (X_filtered - X_filtered.mean(axis=0)) / X_filtered.std(axis=0)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "comp_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=comp_names)\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=comp_names,\n",
    "    index=X_filtered.columns,  \n",
    ")\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed029b30-ec2f-414c-9601-d57e8d67bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  MI Score\n",
      "0     PC1  0.111954\n",
      "1     PC2  0.073373\n",
      "2     PC3  0.043316\n",
      "3     PC4  0.052707\n",
      "4     PC5  0.037834\n"
     ]
    }
   ],
   "source": [
    "mi_scores = mutual_info_classif(X_pca, y)\n",
    "mi_df = pd.DataFrame({'Feature': X_pca.columns, 'MI Score': mi_scores})\n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e196a7-8423-421a-9b95-b40a0aaa5bbc",
   "metadata": {},
   "source": [
    "## Feature Selection and Preprocessing\n",
    "Now, it's time to select the features that we'll use on our prediction. As we saw that the smoking history isn't a particularly good feature to invest in this case, we'll exclude it from the selected features. After that, we'll split our dataset to use 75% of it to training and the other 25% to test our prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d47e155-5365-4003-876a-b52ebfe8bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separetes the Features from the Target.\n",
    "X = df.drop(['diabetes', 'smoking_history'], axis=1)\n",
    "y = df['diabetes']\n",
    "\n",
    "CT = ColumnTransformer(\n",
    "    transformers = [ \n",
    "        ('onehot', OneHotEncoder(sparse_output=False, categories='auto'), ['gender']), #sparse_output=False\n",
    "    ],\t\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Applies the OneHot onto categorical features.\n",
    "X_encoded = CT.fit_transform(X[['gender']])\n",
    "encoded_cols = CT.get_feature_names_out(['gender'])\n",
    "\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_cols, index=X.index)\n",
    "X_features = pd.concat([X.drop(['gender'], axis=1), X_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9b166-3d5e-45cd-ad02-434a42a879d9",
   "metadata": {},
   "source": [
    "As we saw earlier on the EDA, the dataset we're using is imbalanced. The target `diabetes` has the value of 0 much more frequently than 1, with 91.5% of the cases being 0 and 8.5% being 1. In order to ensure that the data will be splitted in a way that keeps the original distribution, we will set it to stratify the split based on the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b52817-612f-41bb-a2b8-b698401baadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e0276-d847-4c18-b9b8-b74dc5a6ae5b",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Here we're going to train a Gradient Boosting with 400 decision trees, fitting it to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c25082-262b-4566-a397-5d983d273ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gboost_baseline = GradientBoostingClassifier(n_estimators = 400, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae90d8-7edd-4ff4-b8fe-823be7b32e45",
   "metadata": {},
   "source": [
    "It is very important to evaluate the classification report wisely. Precision isn't the only thing that cares, even more on this case as we've got a imbalanced dataset. So the recall and f1-score tells us that the model may be predicting a higher number of false negatives than it appears by only looking to the precision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d128abe-132b-4dfb-9d5c-64263031e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     22875\n",
      "           1       0.98      0.68      0.80      2125\n",
      "\n",
      "    accuracy                           0.97     25000\n",
      "   macro avg       0.97      0.84      0.89     25000\n",
      "weighted avg       0.97      0.97      0.97     25000\n",
      "\n",
      "[[22841    34]\n",
      " [  677  1448]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_gboost_baseline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc62d9d-b5bb-40e1-a891-74ee345bf110",
   "metadata": {},
   "source": [
    "In fact, our model gets right 68% of the diabetics people diagnosed. We may trade a bit of false negatives with false positives by changing the confidence that our model has to have to consider a prediction of diabetes as true, using 0.3 of probability as a threshold to predict `True` instead of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1db14-70b6-41f5-879b-4e74a7dce405",
   "metadata": {},
   "source": [
    "#### Adjusting Confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9917fb30-b8a1-4205-b076-44963279b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     22875\n",
      "           1       0.86      0.73      0.79      2125\n",
      "\n",
      "    accuracy                           0.97     25000\n",
      "   macro avg       0.92      0.86      0.88     25000\n",
      "weighted avg       0.97      0.97      0.97     25000\n",
      "\n",
      "[[22621   254]\n",
      " [  579  1546]]\n"
     ]
    }
   ],
   "source": [
    "y_proba = model_gboost_baseline.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.3).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88bb92-bcc6-41b3-af11-9e5fd135a551",
   "metadata": {},
   "source": [
    "In this context, the trade-off is justified because false negatives in medical diagnoses usually tend to represent a bigger problem than false positives. Another alternative is to set weights to each case, aplying lesser values to cases of the class that occurs more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06106a6b-5ace-4690-b1fa-aa40138c69a3",
   "metadata": {},
   "source": [
    "#### Assign Weights\n",
    "After assign weights to each classe, being the weight for class 1 cases five times higher than the weights of cases of class 0, we got even less false positives, almost half of the original amount, in exchange of having near 30 times the amout of false negatives. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b41299-84c7-49b7-93b4-be423d43c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.where(y_train == 1, 5, 1)\n",
    "model_gboost_weighted = GradientBoostingClassifier(n_estimators = 400, random_state=0, n_iter_no_change=10, tol=1e-4).fit(X_train, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf278d9-51be-49bd-b88a-dc903c303691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     22875\n",
      "           1       0.66      0.83      0.73      2125\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.82      0.90      0.85     25000\n",
      "weighted avg       0.96      0.95      0.95     25000\n",
      "\n",
      "[[21950   925]\n",
      " [  358  1767]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_gboost_weighted.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee6c27-b2d3-4602-af2b-a85f195c26d8",
   "metadata": {},
   "source": [
    "#### Undersampling\n",
    "We may employ the undersampling technique to reduce the bias of the model towards predicting the cases as non-diabetic (`False`). \n",
    "In other words, by reducing how many false non-diabetic samples the model sees during its training, we decrease the model tendency to predict values near 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d50f73-3839-429d-8954-b95fb4845e83",
   "metadata": {},
   "source": [
    "First we may repeat the process of splitting the dataset and aplying our column transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf22a9eb-891c-4b06-89e8-92dd29105928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separetes the Features from the Target.\n",
    "df_with_diabetes = df[df['diabetes'] == 1]\n",
    "df_without_diabetes = df[~(df['diabetes'] == 1)]\n",
    "drop_sample_indexes = df_without_diabetes.sample(frac=1/2, random_state=42).index\n",
    "df_u = pd.concat([df_with_diabetes, df_without_diabetes.drop(drop_sample_indexes)], axis=0)\n",
    "\n",
    "X_u = df_u.drop(['diabetes', 'smoking_history'], axis=1)\n",
    "y_u = df_u['diabetes']\n",
    "\n",
    "# Applies the OneHot onto categorical features.\n",
    "X_encoded_u = CT.fit_transform(X_u[['gender']])\n",
    "X_encoded_df_u = pd.DataFrame(X_encoded_u, columns=encoded_cols, index=X_u.index)\n",
    "X_features_u = pd.concat([X_u.drop(['gender'], axis=1), X_encoded_df_u], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55433602-3461-4f57-b61f-141a4f3c30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features_u, y_u, stratify=y_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af4fe-2790-4913-9acb-583f0bcfe7d6",
   "metadata": {},
   "source": [
    "Thus, we train our model again. This time, our non-diabetic samples were reduced by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88fb88ab-975e-45dc-be91-9e6f090138ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gboost_undersampling = GradientBoostingClassifier(n_estimators = 400, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1ba3de8-f6f5-4bca-88c8-02bd6460a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     11438\n",
      "           1       0.93      0.74      0.82      2125\n",
      "\n",
      "    accuracy                           0.95     13563\n",
      "   macro avg       0.94      0.86      0.90     13563\n",
      "weighted avg       0.95      0.95      0.95     13563\n",
      "\n",
      "[[11322   116]\n",
      " [  563  1562]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_gboost_undersampling.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b25f6b3-8969-4bcc-b306-cb9594c28ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96     11438\n",
      "           1       0.79      0.83      0.81      2125\n",
      "\n",
      "    accuracy                           0.94     13563\n",
      "   macro avg       0.88      0.90      0.89     13563\n",
      "weighted avg       0.94      0.94      0.94     13563\n",
      "\n",
      "[[10966   472]\n",
      " [  356  1769]]\n"
     ]
    }
   ],
   "source": [
    "y_proba = model_gboost_undersampling.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.3).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4210e-1913-454f-8df0-bff96fbf65df",
   "metadata": {},
   "source": [
    "## Modeling (Neural Network Alternative)\n",
    "In our first attempt, we used a tree-based model approach: `GradientBoostingClassifier`, which relies on decision trees. As an alternative to tree-based models, deep neural networks (DNN) can be employed. In this case, we're going to use Tensorflow library, so we must first import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52080973-273e-4227-9c2b-6b821d489868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 18:27:45.924039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745270865.940525    9198 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745270865.946119    9198 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745270865.965790    9198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745270865.965811    9198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745270865.965813    9198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745270865.965814    9198 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-21 18:27:45.971242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d55cc-f3bb-4676-8f5d-dc905ce26b97",
   "metadata": {},
   "source": [
    "To ensure we're working with the correct and integral data, we perform a fresh train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b929013d-c9ed-4dc6-acc4-19e676c6ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6df562-fc06-4ec3-94c0-f8bf77496034",
   "metadata": {},
   "source": [
    "Since our problem falls within the classification domain, we must be particularly cautious with multiple metrics beyond accuracy, such as minimizing false positives and maximizing true positives, which are much more relevant to a health-related application such as a diagnostic model. \n",
    "\n",
    "Therefore, we need to define which metrics are the most relevant for this application and keep track of them as the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f2030e5-3ce5-4602-aff5-6a653f093913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745270868.382981    9198 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5529 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "relevant_classification_metrics = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b56dda-3cf1-484f-b058-f967cef79547",
   "metadata": {},
   "source": [
    "Then we define the function that specifies the model's architecture, allowing us to instantiate multiple models to test alternative configurations (as we did with tree-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41e7aa05-d5db-4e2b-9500-4f643c2022e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return tf.keras.Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(units=X_train.shape[-1], activation='linear'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(units=256, activation='linear'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(units=64, activation='linear'),\n",
    "        layers.Dense(units=1, activation='sigmoid'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0d51e-d500-4094-b786-ad694f58ab4e",
   "metadata": {},
   "source": [
    "We must also define some important parameters, such as the batch size (`batch_size`) and the maximum number of epochs(`epochs_max`) for the training. We can set a higher number of epochs, as we will use early stopping to prevent overfitting and avoid spending unnecessary time on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04a4eb5f-3f51-41a5-acd1-e3800f5e35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_max = 100\n",
    "batch_size = 32\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_prc', \n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56869e-95a8-4db6-9852-bda0bc2ee008",
   "metadata": {},
   "source": [
    "Finally, we can call our `build_model()` and compile the DNN model. Fitting a tensorflow model returns a history of the training, which we can keep in order to plot the model's training data (such as the loss on each epoch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dec93-ad8e-480a-a716-9371026e32a3",
   "metadata": {},
   "source": [
    "### Baseline DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "228cdbe6-76cd-4caa-a5db-a98c03f65f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745270871.024443    9267 service.cc:152] XLA service 0x7f3f2c006210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745270871.024506    9267 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2025-04-21 18:27:51.112753: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1745270871.431461    9267 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  30/2344\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.6022 - auc: 0.6103 - fn: 14.3667 - fp: 167.4333 - loss: 0.8084 - prc: 0.1884 - precision: 0.0971 - recall: 0.5314 - tn: 292.9333 - tp: 21.2667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1745270874.154099    9267 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2343/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9015 - auc: 0.8554 - fn: 1748.4370 - fp: 1243.8083 - loss: 0.2526 - prc: 0.4795 - precision: 0.4891 - recall: 0.4524 - tn: 33042.1211 - tp: 1469.6334"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 18:28:09.822407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1712', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 11ms/step - accuracy: 0.9015 - auc: 0.8554 - fn: 1749.8175 - fp: 1244.3569 - loss: 0.2525 - prc: 0.4796 - precision: 0.4892 - recall: 0.4524 - tn: 33070.8594 - tp: 1470.9454 - val_accuracy: 0.9571 - val_auc: 0.9576 - val_fn: 1032.0000 - val_fp: 41.0000 - val_loss: 0.1248 - val_prc: 0.8035 - val_precision: 0.9638 - val_recall: 0.5144 - val_tn: 22834.0000 - val_tp: 1093.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.9463 - auc: 0.9412 - fn: 1491.8989 - fp: 506.1467 - loss: 0.1452 - prc: 0.7133 - precision: 0.7673 - recall: 0.5293 - tn: 33847.1055 - tp: 1690.8273 - val_accuracy: 0.9597 - val_auc: 0.9593 - val_fn: 882.0000 - val_fp: 126.0000 - val_loss: 0.1168 - val_prc: 0.8105 - val_precision: 0.9080 - val_recall: 0.5849 - val_tn: 22749.0000 - val_tp: 1243.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9499 - auc: 0.9433 - fn: 1455.4166 - fp: 442.9160 - loss: 0.1380 - prc: 0.7385 - precision: 0.7970 - recall: 0.5417 - tn: 33922.5898 - tp: 1715.0554 - val_accuracy: 0.9594 - val_auc: 0.9594 - val_fn: 836.0000 - val_fp: 178.0000 - val_loss: 0.1158 - val_prc: 0.8119 - val_precision: 0.8787 - val_recall: 0.6066 - val_tn: 22697.0000 - val_tp: 1289.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9506 - auc: 0.9438 - fn: 1433.2690 - fp: 417.9096 - loss: 0.1375 - prc: 0.7458 - precision: 0.8074 - recall: 0.5493 - tn: 33943.8750 - tp: 1740.9246 - val_accuracy: 0.9599 - val_auc: 0.9595 - val_fn: 837.0000 - val_fp: 166.0000 - val_loss: 0.1155 - val_prc: 0.8124 - val_precision: 0.8858 - val_recall: 0.6061 - val_tn: 22709.0000 - val_tp: 1288.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.9510 - auc: 0.9440 - fn: 1440.9692 - fp: 413.1825 - loss: 0.1365 - prc: 0.7415 - precision: 0.8082 - recall: 0.5429 - tn: 33954.5430 - tp: 1727.2853 - val_accuracy: 0.9596 - val_auc: 0.9590 - val_fn: 820.0000 - val_fp: 190.0000 - val_loss: 0.1157 - val_prc: 0.8113 - val_precision: 0.8729 - val_recall: 0.6141 - val_tn: 22685.0000 - val_tp: 1305.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9496 - auc: 0.9450 - fn: 1458.4951 - fp: 424.2217 - loss: 0.1384 - prc: 0.7509 - precision: 0.8075 - recall: 0.5506 - tn: 33892.7109 - tp: 1760.5514 - val_accuracy: 0.9595 - val_auc: 0.9590 - val_fn: 872.0000 - val_fp: 140.0000 - val_loss: 0.1160 - val_prc: 0.8124 - val_precision: 0.8995 - val_recall: 0.5896 - val_tn: 22735.0000 - val_tp: 1253.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9509 - auc: 0.9432 - fn: 1436.6653 - fp: 404.0119 - loss: 0.1373 - prc: 0.7457 - precision: 0.8133 - recall: 0.5459 - tn: 33951.9844 - tp: 1743.3164 - val_accuracy: 0.9596 - val_auc: 0.9598 - val_fn: 863.0000 - val_fp: 146.0000 - val_loss: 0.1163 - val_prc: 0.8123 - val_precision: 0.8963 - val_recall: 0.5939 - val_tn: 22729.0000 - val_tp: 1262.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9496 - auc: 0.9451 - fn: 1462.8687 - fp: 414.6166 - loss: 0.1377 - prc: 0.7420 - precision: 0.8058 - recall: 0.5365 - tn: 33937.4414 - tp: 1721.0511 - val_accuracy: 0.9600 - val_auc: 0.9591 - val_fn: 899.0000 - val_fp: 102.0000 - val_loss: 0.1171 - val_prc: 0.8120 - val_precision: 0.9232 - val_recall: 0.5769 - val_tn: 22773.0000 - val_tp: 1226.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9501 - auc: 0.9456 - fn: 1448.9531 - fp: 400.8371 - loss: 0.1367 - prc: 0.7450 - precision: 0.8098 - recall: 0.5384 - tn: 33955.6602 - tp: 1730.5309 - val_accuracy: 0.9595 - val_auc: 0.9596 - val_fn: 779.0000 - val_fp: 234.0000 - val_loss: 0.1167 - val_prc: 0.8124 - val_precision: 0.8519 - val_recall: 0.6334 - val_tn: 22641.0000 - val_tp: 1346.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9515 - auc: 0.9429 - fn: 1401.7476 - fp: 406.9791 - loss: 0.1357 - prc: 0.7516 - precision: 0.8088 - recall: 0.5565 - tn: 33964.3242 - tp: 1762.9305 - val_accuracy: 0.9589 - val_auc: 0.9600 - val_fn: 895.0000 - val_fp: 132.0000 - val_loss: 0.1173 - val_prc: 0.8118 - val_precision: 0.9031 - val_recall: 0.5788 - val_tn: 22743.0000 - val_tp: 1230.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9508 - auc: 0.9461 - fn: 1436.2064 - fp: 415.6797 - loss: 0.1355 - prc: 0.7493 - precision: 0.8102 - recall: 0.5468 - tn: 33926.2734 - tp: 1757.8213 - val_accuracy: 0.9594 - val_auc: 0.9601 - val_fn: 837.0000 - val_fp: 179.0000 - val_loss: 0.1163 - val_prc: 0.8121 - val_precision: 0.8780 - val_recall: 0.6061 - val_tn: 22696.0000 - val_tp: 1288.0000\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    }
   ],
   "source": [
    "model_dnn_baseline = build_model()\n",
    "\n",
    "model_dnn_baseline.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=relevant_classification_metrics\n",
    ")\n",
    "\n",
    "history_baseline = model_dnn_baseline.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs_max,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b2421-fc31-482f-84b2-576323635236",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Now we make predictions using the DNN-based model to evaluate its performance on the validation data. Comparing the results, even when changing the confidence threshold, we observed a slightly worse performance from the DNN-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62756eb1-bcb5-4ceb-9bb1-78dd9d8dfa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     22875\n",
      "           1       0.90      0.59      0.71      2125\n",
      "\n",
      "    accuracy                           0.96     25000\n",
      "   macro avg       0.93      0.79      0.85     25000\n",
      "weighted avg       0.96      0.96      0.96     25000\n",
      "\n",
      "[[22735   140]\n",
      " [  872  1253]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_baseline.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36bae2bc-cd2d-4c6f-b4f2-e26f3f6ab068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     22875\n",
      "           1       0.77      0.69      0.73      2125\n",
      "\n",
      "    accuracy                           0.96     25000\n",
      "   macro avg       0.87      0.83      0.85     25000\n",
      "weighted avg       0.95      0.96      0.95     25000\n",
      "\n",
      "[[22440   435]\n",
      " [  668  1457]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_baseline.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.3).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df14e9-a4d7-4fb1-9c99-6444c939e634",
   "metadata": {},
   "source": [
    "\n",
    "Therefore, if we had a larger dataset, the DNN approach would likely outperform the tree-based model due to its ability to learn more complex patterns. Given this characteristic, we will not repeat the undersampling test previously applied on the Tree-based approach. However, we will test an alternative version of the model trained with different class weights, as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49a2dc-465a-4a10-8d37-ebf6090e8ea5",
   "metadata": {},
   "source": [
    "### Alternative DNN Model (Using class weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deb6787e-2150-4de4-98ca-a3f897071d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9ms/step - accuracy: 0.9154 - auc: 0.9250 - fn: 1656.4418 - fp: 3802.2798 - loss: 0.5098 - prc: 0.6197 - precision: 0.5054 - recall: 0.6768 - tn: 53449.6836 - tp: 3627.5732 - val_accuracy: 0.9222 - val_auc: 0.9590 - val_fn: 391.0000 - val_fp: 1555.0000 - val_loss: 0.1925 - val_prc: 0.8029 - val_precision: 0.5272 - val_recall: 0.8160 - val_tn: 21320.0000 - val_tp: 1734.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9115 - auc: 0.9448 - fn: 723.2631 - fp: 2637.4797 - loss: 0.3701 - prc: 0.7132 - precision: 0.4874 - recall: 0.7776 - tn: 31703.1582 - tp: 2472.0784 - val_accuracy: 0.9455 - val_auc: 0.9602 - val_fn: 555.0000 - val_fp: 808.0000 - val_loss: 0.1636 - val_prc: 0.8113 - val_precision: 0.6602 - val_recall: 0.7388 - val_tn: 22067.0000 - val_tp: 1570.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9159 - auc: 0.9464 - fn: 728.2039 - fp: 2445.8154 - loss: 0.3613 - prc: 0.7319 - precision: 0.5020 - recall: 0.7682 - tn: 31895.0039 - tp: 2466.9573 - val_accuracy: 0.9146 - val_auc: 0.9595 - val_fn: 370.0000 - val_fp: 1764.0000 - val_loss: 0.1998 - val_prc: 0.8083 - val_precision: 0.4987 - val_recall: 0.8259 - val_tn: 21111.0000 - val_tp: 1755.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9145 - auc: 0.9462 - fn: 721.9139 - fp: 2463.5825 - loss: 0.3644 - prc: 0.7311 - precision: 0.5006 - recall: 0.7753 - tn: 31840.8047 - tp: 2509.6785 - val_accuracy: 0.9274 - val_auc: 0.9600 - val_fn: 433.0000 - val_fp: 1383.0000 - val_loss: 0.1724 - val_prc: 0.8120 - val_precision: 0.5502 - val_recall: 0.7962 - val_tn: 21492.0000 - val_tp: 1692.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9174 - auc: 0.9502 - fn: 687.0601 - fp: 2424.6235 - loss: 0.3500 - prc: 0.7423 - precision: 0.5091 - recall: 0.7898 - tn: 31921.4531 - tp: 2502.8430 - val_accuracy: 0.9290 - val_auc: 0.9599 - val_fn: 432.0000 - val_fp: 1343.0000 - val_loss: 0.1834 - val_prc: 0.8130 - val_precision: 0.5576 - val_recall: 0.7967 - val_tn: 21532.0000 - val_tp: 1693.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9186 - auc: 0.9482 - fn: 697.6307 - fp: 2361.3884 - loss: 0.3577 - prc: 0.7447 - precision: 0.5179 - recall: 0.7833 - tn: 31967.4766 - tp: 2509.4844 - val_accuracy: 0.9297 - val_auc: 0.9603 - val_fn: 438.0000 - val_fp: 1319.0000 - val_loss: 0.1728 - val_prc: 0.8107 - val_precision: 0.5612 - val_recall: 0.7939 - val_tn: 21556.0000 - val_tp: 1687.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - accuracy: 0.9197 - auc: 0.9480 - fn: 682.2188 - fp: 2349.2158 - loss: 0.3537 - prc: 0.7329 - precision: 0.5119 - recall: 0.7839 - tn: 32039.5957 - tp: 2464.9497 - val_accuracy: 0.9304 - val_auc: 0.9601 - val_fn: 435.0000 - val_fp: 1305.0000 - val_loss: 0.1770 - val_prc: 0.8100 - val_precision: 0.5643 - val_recall: 0.7953 - val_tn: 21570.0000 - val_tp: 1690.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.9178 - auc: 0.9501 - fn: 711.1138 - fp: 2386.2200 - loss: 0.3526 - prc: 0.7434 - precision: 0.5144 - recall: 0.7798 - tn: 31938.6465 - tp: 2500.0000 - val_accuracy: 0.9216 - val_auc: 0.9599 - val_fn: 398.0000 - val_fp: 1561.0000 - val_loss: 0.1814 - val_prc: 0.8080 - val_precision: 0.5252 - val_recall: 0.8127 - val_tn: 21314.0000 - val_tp: 1727.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 10ms/step - accuracy: 0.9210 - auc: 0.9497 - fn: 697.7104 - fp: 2319.3091 - loss: 0.3467 - prc: 0.7431 - precision: 0.5162 - recall: 0.7839 - tn: 32068.5957 - tp: 2450.3650 - val_accuracy: 0.9293 - val_auc: 0.9600 - val_fn: 433.0000 - val_fp: 1334.0000 - val_loss: 0.1718 - val_prc: 0.8066 - val_precision: 0.5592 - val_recall: 0.7962 - val_tn: 21541.0000 - val_tp: 1692.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.9187 - auc: 0.9495 - fn: 704.3045 - fp: 2347.8870 - loss: 0.3514 - prc: 0.7394 - precision: 0.5126 - recall: 0.7772 - tn: 31989.9004 - tp: 2493.8875 - val_accuracy: 0.9301 - val_auc: 0.9603 - val_fn: 430.0000 - val_fp: 1317.0000 - val_loss: 0.1725 - val_prc: 0.8114 - val_precision: 0.5627 - val_recall: 0.7976 - val_tn: 21558.0000 - val_tp: 1695.0000\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    }
   ],
   "source": [
    "model_dnn_weighted = build_model()\n",
    "\n",
    "model_dnn_weighted.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=relevant_classification_metrics\n",
    ")\n",
    "\n",
    "history_weighted = model_dnn_weighted.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs_max,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight={0: 1.0, 1: 5.0} # Applies a 5x higher weight to diabetic samples.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226bfd4-2b53-43f5-8072-ea58c243f1c5",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "As we saw before, with the tree-based models, the class weights in fact reduced the false negatives with the trade-off of incresing the false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e42ccb8-1c53-47b9-97f0-526ed80bab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     22875\n",
      "           1       0.56      0.80      0.66      2125\n",
      "\n",
      "    accuracy                           0.93     25000\n",
      "   macro avg       0.77      0.87      0.81     25000\n",
      "weighted avg       0.94      0.93      0.93     25000\n",
      "\n",
      "[[21532  1343]\n",
      " [  432  1693]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_weighted.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bad5c171-57b6-4093-9d65-75dabd4a30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92     22875\n",
      "           1       0.37      0.90      0.53      2125\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.68      0.88      0.72     25000\n",
      "weighted avg       0.94      0.86      0.89     25000\n",
      "\n",
      "[[19652  3223]\n",
      " [  219  1906]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_weighted.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.3).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91e141-d419-41d0-b949-45cec64669bf",
   "metadata": {},
   "source": [
    "### Models... Ensemble!\n",
    "Now that we have multiple models, we are going to create an ensemble using all them (both tree-based and DNN-based). To prevent data leakage, we should first update our training code to ensure that all the models are trained using the same train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbe4b7-4be4-4af9-8744-62f4197bf084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
