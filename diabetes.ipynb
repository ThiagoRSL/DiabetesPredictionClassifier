{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6175060f-397f-49a9-9024-935956fc0a6c",
   "metadata": {},
   "source": [
    "## Setting up to Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc20c5-73de-4fd1-a0c3-e8dcf96597af",
   "metadata": {},
   "source": [
    "The first part of the process, importing the libraries and depend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538b360e-c563-4015-9098-e0bc317112cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3b688-a325-475d-b92c-b1c3b199edab",
   "metadata": {},
   "source": [
    "Loading dataset, after downloading it from kaggle: https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9b17e0-25b2-4cda-b3f2-4d431c909962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"diabetes_prediction_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6746e-eb6f-4067-90b9-db8d7b9cc397",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c868351-8a3d-4d36-8540-6f0db7fac962",
   "metadata": {},
   "source": [
    "The first steps are to understand the data, in order to do so, first we may get some general information about the dataset through the `head()`, `info()` and `describe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38abd20b-56ba-48c2-90ab-3274ef629a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>smoking_history</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42197</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57911</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.59</td>\n",
       "      <td>7.0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76268</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>22.46</td>\n",
       "      <td>6.5</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79982</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>40.33</td>\n",
       "      <td>8.2</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82655</th>\n",
       "      <td>Male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>31.41</td>\n",
       "      <td>6.2</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
       "42197   Male  23.0             0              0         No Info  27.32   \n",
       "57911   Male  23.0             0              0         No Info  27.59   \n",
       "76268   Male  23.0             0              0           never  22.46   \n",
       "79982   Male  23.0             0              0         No Info  40.33   \n",
       "82655   Male  23.0             0              0           never  31.41   \n",
       "\n",
       "       HbA1c_level  blood_glucose_level  diabetes  \n",
       "42197          5.7                  145         1  \n",
       "57911          7.0                  159         1  \n",
       "76268          6.5                  140         1  \n",
       "79982          8.2                  220         1  \n",
       "82655          6.2                  140         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['gender'] == 'Male') & (df['age'] == 23.0) & (df['diabetes'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13223a1-8174-4d70-957f-42db0229d841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>smoking_history</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>never</td>\n",
       "      <td>25.19</td>\n",
       "      <td>6.6</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No Info</td>\n",
       "      <td>27.32</td>\n",
       "      <td>6.6</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>never</td>\n",
       "      <td>27.32</td>\n",
       "      <td>5.7</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>current</td>\n",
       "      <td>23.45</td>\n",
       "      <td>5.0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>current</td>\n",
       "      <td>20.14</td>\n",
       "      <td>4.8</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender   age  hypertension  heart_disease smoking_history    bmi  \\\n",
       "0  Female  80.0             0              1           never  25.19   \n",
       "1  Female  54.0             0              0         No Info  27.32   \n",
       "2    Male  28.0             0              0           never  27.32   \n",
       "3  Female  36.0             0              0         current  23.45   \n",
       "4    Male  76.0             1              1         current  20.14   \n",
       "\n",
       "   HbA1c_level  blood_glucose_level  diabetes  \n",
       "0          6.6                  140         0  \n",
       "1          6.6                   80         0  \n",
       "2          5.7                  158         0  \n",
       "3          5.0                  155         0  \n",
       "4          4.8                  155         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abee033c-4469-41c0-9b9a-d904f719f3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   gender               100000 non-null  object \n",
      " 1   age                  100000 non-null  float64\n",
      " 2   hypertension         100000 non-null  int64  \n",
      " 3   heart_disease        100000 non-null  int64  \n",
      " 4   smoking_history      100000 non-null  object \n",
      " 5   bmi                  100000 non-null  float64\n",
      " 6   HbA1c_level          100000 non-null  float64\n",
      " 7   blood_glucose_level  100000 non-null  int64  \n",
      " 8   diabetes             100000 non-null  int64  \n",
      "dtypes: float64(3), int64(4), object(2)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c05d5f0-e371-4ddb-8b30-aee1bb320204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>heart_disease</th>\n",
       "      <th>bmi</th>\n",
       "      <th>HbA1c_level</th>\n",
       "      <th>blood_glucose_level</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.885856</td>\n",
       "      <td>0.07485</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>27.320767</td>\n",
       "      <td>5.527507</td>\n",
       "      <td>138.058060</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>22.516840</td>\n",
       "      <td>0.26315</td>\n",
       "      <td>0.194593</td>\n",
       "      <td>6.636783</td>\n",
       "      <td>1.070672</td>\n",
       "      <td>40.708136</td>\n",
       "      <td>0.278883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.010000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.630000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.320000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.580000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.690000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age  hypertension  heart_disease            bmi  \\\n",
       "count  100000.000000  100000.00000  100000.000000  100000.000000   \n",
       "mean       41.885856       0.07485       0.039420      27.320767   \n",
       "std        22.516840       0.26315       0.194593       6.636783   \n",
       "min         0.080000       0.00000       0.000000      10.010000   \n",
       "25%        24.000000       0.00000       0.000000      23.630000   \n",
       "50%        43.000000       0.00000       0.000000      27.320000   \n",
       "75%        60.000000       0.00000       0.000000      29.580000   \n",
       "max        80.000000       1.00000       1.000000      95.690000   \n",
       "\n",
       "         HbA1c_level  blood_glucose_level       diabetes  \n",
       "count  100000.000000        100000.000000  100000.000000  \n",
       "mean        5.527507           138.058060       0.085000  \n",
       "std         1.070672            40.708136       0.278883  \n",
       "min         3.500000            80.000000       0.000000  \n",
       "25%         4.800000           100.000000       0.000000  \n",
       "50%         5.800000           140.000000       0.000000  \n",
       "75%         6.200000           159.000000       0.000000  \n",
       "max         9.000000           300.000000       1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c06bb65-5129-4e80-8939-97b875983abd",
   "metadata": {},
   "source": [
    "So, after a brief analysis we may conclude:\n",
    "- There are a plentiful number of data available, 100.000 cases.\n",
    "- The data doesn't contain any explicit error-leading missing values such as NAs, but it has the `No info` class of `smoking_history`.\n",
    "- There are two categorical variables, `gender` and `smoking_history`, both will have to be transformed to a numerical value.\n",
    "- `diabetes`, the target, is a boolean value and have a mean of 0.085. Meaning that only 8.5% of the cases in fact have diabetes implying on a imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1582abb-8484-48a9-b304-63a1644bf571",
   "metadata": {},
   "source": [
    "Next, we going to analyse the `smoking_history` feature as it appears to be problematic for being a categorical feature with missing values. \n",
    "\n",
    "The first step is to check the possible values this feature can take and their respective frequencies. As shown in the next cell, this feature has a couple of issues:\n",
    "- `No Info` appears in 35916 cases, meaning that more than one-third of the cases has a unespecified value on this feature (missing data).\n",
    "- There are ambiguous and overlapping categories. For example, `not current` could mean the same as `former` or `never`, and the criteria that distinguise `ever` from `current` is poorly defined.\n",
    "\n",
    "Considering the high number of unkown values and the ambiguity in class definition, a further evaluations is needed to assess this feature impact on the target prediction. This will help justify the efford of keeping this feature, or determine if it should be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9d3902-750c-45df-9779-876ad189901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smoking_history\n",
       "No Info        35816\n",
       "never          35095\n",
       "former          9352\n",
       "current         9286\n",
       "not current     6447\n",
       "ever            4004\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['smoking_history'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac6d59-a7bb-41e2-896e-8e5cd6df8018",
   "metadata": {},
   "source": [
    "In order to further evaluate, we'll define a `ColumnTransformer` and transform the categorical values through the One-Hot Encoding. With the categorical data transformed, we can then use some feature metrics, such as Mutual Information score and correlation, to understand the features relevance to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a609ce8-983e-4a87-91da-ca21434e06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separetes the Features from the Target.\n",
    "X = df.drop(['diabetes'], axis=1)\n",
    "y = df['diabetes']\n",
    "\n",
    "CT = ColumnTransformer(\n",
    "    transformers = [ \n",
    "        ('onehot', OneHotEncoder(sparse_output=False, categories='auto'), ['gender', 'smoking_history']), #sparse_output=False\n",
    "        #('ordinal', OrdinalEncoder(categories=[['never','No Info', 'not current', 'former', 'current', 'ever']]), ['smoking_history']) #sparse_output=False\n",
    "    ],\t\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8805fe42-bb18-48ef-aa12-ae9b6bf8cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the OneHot onto categorical features.\n",
    "X_encoded = CT.fit_transform(X[['gender', 'smoking_history']])\n",
    "encoded_cols = CT.get_feature_names_out(['gender', 'smoking_history'])\n",
    "\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_cols, index=X.index)\n",
    "X_features = pd.concat([X.drop(['gender', 'smoking_history'], axis=1), X_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee96c9e-c566-4389-ab75-f04abad7d540",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "Analyzing the correlation between the features and the target variable, allows us to see which of the features are most strong linear related to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605874fa-716a-4796-961e-6e79eeed724b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diabetes                               1.000000\n",
       "blood_glucose_level                    0.419558\n",
       "HbA1c_level                            0.400660\n",
       "age                                    0.258008\n",
       "bmi                                    0.214357\n",
       "hypertension                           0.197823\n",
       "heart_disease                          0.171727\n",
       "onehot__smoking_history_former         0.097917\n",
       "onehot__gender_Male                    0.037666\n",
       "onehot__smoking_history_never          0.027267\n",
       "onehot__smoking_history_ever           0.024080\n",
       "onehot__smoking_history_not current    0.020734\n",
       "onehot__smoking_history_current        0.019606\n",
       "onehot__gender_Other                  -0.004090\n",
       "onehot__gender_Female                 -0.037553\n",
       "onehot__smoking_history_No Info       -0.118939\n",
       "Name: diabetes, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trans = pd.concat([X_features, y], axis=1)\n",
    "df_trans.corr()['diabetes'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c649e1-3c5d-4d5e-9976-3ee051b54584",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "Mutual Information can capture many types of relationships that each variable may have with the target, not being limited by linear associations only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f5c4fa-6374-4697-a547-16abc5d127b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Feature  MI Score\n",
      "4                           HbA1c_level  0.131731\n",
      "5                   blood_glucose_level  0.113434\n",
      "0                                   age  0.041489\n",
      "3                                   bmi  0.025702\n",
      "9       onehot__smoking_history_No Info  0.016061\n",
      "6                 onehot__gender_Female  0.015179\n",
      "1                          hypertension  0.014148\n",
      "7                   onehot__gender_Male  0.010017\n",
      "2                         heart_disease  0.007221\n",
      "13        onehot__smoking_history_never  0.005660\n",
      "12       onehot__smoking_history_former  0.004894\n",
      "8                  onehot__gender_Other  0.001077\n",
      "10      onehot__smoking_history_current  0.000918\n",
      "11         onehot__smoking_history_ever  0.000000\n",
      "14  onehot__smoking_history_not current  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Calcula a MI\n",
    "mi_scores = mutual_info_classif(X_features, y)\n",
    "\n",
    "# Exibe os resultados\n",
    "mi_df = pd.DataFrame({'Feature': X_features.columns, 'MI Score': mi_scores})\n",
    "print(mi_df.sort_values(by='MI Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23902358-acad-4d6a-8c50-8ab99f582478",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Using PCA we may obtain valueble information about components made using the linear combination of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d483e3b-6bed-49d6-b650-d731bb891e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          PC1       PC2       PC3       PC4       PC5\n",
      "age                  0.566264 -0.265468 -0.057950 -0.156845 -0.023003\n",
      "hypertension         0.421370 -0.175895  0.029841  0.864849  0.076444\n",
      "heart_disease        0.352297 -0.143349  0.818735 -0.296400 -0.018912\n",
      "bmi                  0.456550 -0.218234 -0.569784 -0.360819 -0.056236\n",
      "HbA1c_level          0.284349  0.656053  0.000659  0.058274 -0.696617\n",
      "blood_glucose_level  0.297298  0.632461 -0.027746 -0.077440  0.710515\n"
     ]
    }
   ],
   "source": [
    "X_filtered = X_features[[\"age\", \"hypertension\", \"heart_disease\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\"]]\n",
    "X_scaled = (X_filtered - X_filtered.mean(axis=0)) / X_filtered.std(axis=0)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "comp_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=comp_names)\n",
    "\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=comp_names,\n",
    "    index=X_filtered.columns,  \n",
    ")\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed029b30-ec2f-414c-9601-d57e8d67bb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  MI Score\n",
      "0     PC1  0.112008\n",
      "1     PC2  0.073391\n",
      "2     PC3  0.043280\n",
      "3     PC4  0.052672\n",
      "4     PC5  0.037825\n"
     ]
    }
   ],
   "source": [
    "mi_scores = mutual_info_classif(X_pca, y)\n",
    "mi_df = pd.DataFrame({'Feature': X_pca.columns, 'MI Score': mi_scores})\n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e196a7-8423-421a-9b95-b40a0aaa5bbc",
   "metadata": {},
   "source": [
    "## Feature Selection and Preprocessing\n",
    "Now, it's time to select the features that we'll use on our prediction. As we saw that the smoking history isn't a particularly good feature to invest in this case, we'll exclude it from the selected features. After that, we'll split our dataset to use 75% of it to training and the other 25% to test our prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d47e155-5365-4003-876a-b52ebfe8bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separetes the Features from the Target.\n",
    "X = df.drop(['diabetes', 'smoking_history'], axis=1)\n",
    "y = df['diabetes']\n",
    "\n",
    "CT = ColumnTransformer(\n",
    "    transformers = [ \n",
    "        ('onehot', OneHotEncoder(sparse_output=False, categories='auto'), ['gender']), #sparse_output=False\n",
    "    ],\t\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Applies the OneHot onto categorical features.\n",
    "X_encoded = CT.fit_transform(X[['gender']])\n",
    "encoded_cols = CT.get_feature_names_out(['gender'])\n",
    "\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_cols, index=X.index)\n",
    "X_features = pd.concat([X.drop(['gender'], axis=1), X_encoded_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9b166-3d5e-45cd-ad02-434a42a879d9",
   "metadata": {},
   "source": [
    "As we saw earlier on the EDA, the dataset we're using is imbalanced. The target `diabetes` has the value of 0 much more frequently than 1, with 91.5% of the cases being 0 and 8.5% being 1. In order to ensure that the data will be splitted in a way that keeps the original distribution, we will set it to stratify the split based on the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b52817-612f-41bb-a2b8-b698401baadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e0276-d847-4c18-b9b8-b74dc5a6ae5b",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "Here we're going to train a Gradient Boosting with 400 decision trees, fitting it to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c25082-262b-4566-a397-5d983d273ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 400, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae90d8-7edd-4ff4-b8fe-823be7b32e45",
   "metadata": {},
   "source": [
    "It is very important to evaluate the classification report wisely. Precision isn't the only thing that cares, even more on this case as we've got a imbalanced dataset. So the recall and f1-score tells us that the model may be predicting a higher number of false negatives than it appears by only looking to the precision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d128abe-132b-4dfb-9d5c-64263031e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     22875\n",
      "           1       0.96      0.68      0.80      2125\n",
      "\n",
      "    accuracy                           0.97     25000\n",
      "   macro avg       0.97      0.84      0.89     25000\n",
      "weighted avg       0.97      0.97      0.97     25000\n",
      "\n",
      "[[22819    56]\n",
      " [  678  1447]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc62d9d-b5bb-40e1-a891-74ee345bf110",
   "metadata": {},
   "source": [
    "In fact, our model gets right 68% of the diabetics people diagnosed. We may trade a bit of false negatives with false positives by changing the confidence that our model has to have to consider a prediction of diabetes as true, using 0.3 of probability as a threshold to predict `True` instead of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c1db14-70b6-41f5-879b-4e74a7dce405",
   "metadata": {},
   "source": [
    "#### Adjusting Confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9917fb30-b8a1-4205-b076-44963279b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     22875\n",
      "           1       0.86      0.72      0.79      2125\n",
      "\n",
      "    accuracy                           0.97     25000\n",
      "   macro avg       0.92      0.86      0.88     25000\n",
      "weighted avg       0.97      0.97      0.97     25000\n",
      "\n",
      "[[22626   249]\n",
      " [  585  1540]]\n"
     ]
    }
   ],
   "source": [
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.3).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88bb92-bcc6-41b3-af11-9e5fd135a551",
   "metadata": {},
   "source": [
    "In this context, the trade-off is justified because false negatives in medical diagnoses usually tend to represent a bigger problem than false positives. Another alternative is to set weights to each case, aplying lesser values to cases of the class that occurs more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06106a6b-5ace-4690-b1fa-aa40138c69a3",
   "metadata": {},
   "source": [
    "#### Assign Weights\n",
    "After assign weights to each classe, being the weight for class 1 cases five times higher than the weights of cases of class 0, we got even less false positives, almost half of the original amount, in exchange of having near 30 times the amout of false negatives. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b41299-84c7-49b7-93b4-be423d43c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.where(y_train == 1, 5, 1)\n",
    "model = GradientBoostingClassifier(n_estimators = 400, random_state=0, n_iter_no_change=10, tol=1e-4).fit(X_train, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbf278d9-51be-49bd-b88a-dc903c303691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97     22875\n",
      "           1       0.66      0.82      0.73      2125\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.82      0.89      0.85     25000\n",
      "weighted avg       0.96      0.95      0.95     25000\n",
      "\n",
      "[[21964   911]\n",
      " [  379  1746]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee6c27-b2d3-4602-af2b-a85f195c26d8",
   "metadata": {},
   "source": [
    "#### Undersampling\n",
    "We may employ the undersampling technique to reduce the bias of the model towards predicting the cases as non-diabetic (`False`). \n",
    "In other words, by reducing how many false non-diabetic samples the model sees during its training, we decrease the model tendency to predict values near 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d50f73-3839-429d-8954-b95fb4845e83",
   "metadata": {},
   "source": [
    "First we may repeat the process of splitting the dataset and aplying our column transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf22a9eb-891c-4b06-89e8-92dd29105928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separetes the Features from the Target.\n",
    "df_with_diabetes = df[df['diabetes'] == 1]\n",
    "df_without_diabetes = df[~(df['diabetes'] == 1)]\n",
    "drop_sample_indexes = df_without_diabetes.sample(frac=1/2, random_state=42).index\n",
    "df_u = pd.concat([df_with_diabetes, df_without_diabetes.drop(drop_sample_indexes)], axis=0)\n",
    "\n",
    "X_u = df_u.drop(['diabetes', 'smoking_history'], axis=1)\n",
    "y_u = df_u['diabetes']\n",
    "\n",
    "# Applies the OneHot onto categorical features.\n",
    "X_encoded_u = CT.fit_transform(X_u[['gender']])\n",
    "X_encoded_df_u = pd.DataFrame(X_encoded_u, columns=encoded_cols, index=X_u.index)\n",
    "X_features_u = pd.concat([X_u.drop(['gender'], axis=1), X_encoded_df_u], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55433602-3461-4f57-b61f-141a4f3c30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features_u, y_u, stratify=y_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11af4fe-2790-4913-9acb-583f0bcfe7d6",
   "metadata": {},
   "source": [
    "Thus, we train our model again. This time, our non-diabetic samples were reduced by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88fb88ab-975e-45dc-be91-9e6f090138ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators = 400, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1ba3de8-f6f5-4bca-88c8-02bd6460a86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     11438\n",
      "           1       0.94      0.73      0.82      2125\n",
      "\n",
      "    accuracy                           0.95     13563\n",
      "   macro avg       0.94      0.86      0.90     13563\n",
      "weighted avg       0.95      0.95      0.95     13563\n",
      "\n",
      "[[11335   103]\n",
      " [  578  1547]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b25f6b3-8969-4bcc-b306-cb9594c28ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97     11438\n",
      "           1       0.81      0.83      0.82      2125\n",
      "\n",
      "    accuracy                           0.94     13563\n",
      "   macro avg       0.89      0.90      0.89     13563\n",
      "weighted avg       0.94      0.94      0.94     13563\n",
      "\n",
      "[[11015   423]\n",
      " [  361  1764]]\n"
     ]
    }
   ],
   "source": [
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = (y_proba >= 0.3).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4210e-1913-454f-8df0-bff96fbf65df",
   "metadata": {},
   "source": [
    "## Modeling (Neural Network Alternative)\n",
    "In our first attempt, we used a tree-based model approach: `GradientBoostingClassifier`, which relies on decision trees. As an alternative to tree-based models, deep neural networks (DNN) can be employed. In this case, we're going to use Tensorflow library, so we must first import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52080973-273e-4227-9c2b-6b821d489868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d55cc-f3bb-4676-8f5d-dc905ce26b97",
   "metadata": {},
   "source": [
    "To ensure we're working with the correct and integral data, we perform a fresh train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b929013d-c9ed-4dc6-acc4-19e676c6ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6df562-fc06-4ec3-94c0-f8bf77496034",
   "metadata": {},
   "source": [
    "Since our problem falls within the classification domain, we must be particularly cautious with multiple metrics beyond accuracy, such as minimizing false positives and maximizing true positives, which are much more relevant to a health-related application such as a diagnostic model. \n",
    "\n",
    "Therefore, we need to define which metrics are the most relevant for this application and keep track of them as the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f2030e5-3ce5-4602-aff5-6a653f093913",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_classification_metrics = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b56dda-3cf1-484f-b058-f967cef79547",
   "metadata": {},
   "source": [
    "Then we define the function that specifies the model's architecture, allowing us to instantiate multiple models to test alternative configurations (as we did with tree-based models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41e7aa05-d5db-4e2b-9500-4f643c2022e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    return tf.keras.Sequential([\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(units=X_train.shape[-1], activation='linear'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(units=128, activation='linear'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(units=64, activation='linear'),\n",
    "        layers.Dense(units=1, activation='sigmoid'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0d51e-d500-4094-b786-ad694f58ab4e",
   "metadata": {},
   "source": [
    "We must also define some important parameters, such as the batch size (`batch_size`) and the maximum number of epochs(`epochs_max`) for the training. We can set a higher number of epochs, as we will use early stopping to prevent overfitting and avoid spending unnecessary time on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04a4eb5f-3f51-41a5-acd1-e3800f5e35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_max = 100\n",
    "batch_size = 32\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_prc', \n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56869e-95a8-4db6-9852-bda0bc2ee008",
   "metadata": {},
   "source": [
    "Finally, we can call our `build_model()` and compile the DNN model. Fitting a tensorflow model returns a history of the training, which we can keep in order to plot the model's training data (such as the loss on each epoch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499dec93-ad8e-480a-a716-9371026e32a3",
   "metadata": {},
   "source": [
    "### Baseline DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "228cdbe6-76cd-4caa-a5db-a98c03f65f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10ms/step - accuracy: 0.8982 - auc: 0.8378 - fn: 1867.6750 - fp: 1226.9778 - loss: 0.2577 - prc: 0.4367 - precision: 0.4603 - recall: 0.4056 - tn: 33124.0898 - tp: 1317.2375 - val_accuracy: 0.9523 - val_auc: 0.9576 - val_fn: 1174.0000 - val_fp: 18.0000 - val_loss: 0.1325 - val_prc: 0.8050 - val_precision: 0.9814 - val_recall: 0.4475 - val_tn: 22857.0000 - val_tp: 951.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9448 - auc: 0.9372 - fn: 1572.6025 - fp: 497.1476 - loss: 0.1494 - prc: 0.7020 - precision: 0.7665 - recall: 0.5091 - tn: 33833.6484 - tp: 1632.5791 - val_accuracy: 0.9588 - val_auc: 0.9579 - val_fn: 752.0000 - val_fp: 278.0000 - val_loss: 0.1177 - val_prc: 0.8074 - val_precision: 0.8316 - val_recall: 0.6461 - val_tn: 22597.0000 - val_tp: 1373.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9451 - auc: 0.9381 - fn: 1564.5131 - fp: 468.7446 - loss: 0.1467 - prc: 0.7113 - precision: 0.7698 - recall: 0.5084 - tn: 33848.8516 - tp: 1653.8691 - val_accuracy: 0.9599 - val_auc: 0.9574 - val_fn: 868.0000 - val_fp: 134.0000 - val_loss: 0.1180 - val_prc: 0.8067 - val_precision: 0.9037 - val_recall: 0.5915 - val_tn: 22741.0000 - val_tp: 1257.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9490 - auc: 0.9440 - fn: 1473.5731 - fp: 438.2524 - loss: 0.1380 - prc: 0.7449 - precision: 0.7964 - recall: 0.5423 - tn: 33900.7109 - tp: 1723.4448 - val_accuracy: 0.9598 - val_auc: 0.9585 - val_fn: 869.0000 - val_fp: 137.0000 - val_loss: 0.1182 - val_prc: 0.8083 - val_precision: 0.9017 - val_recall: 0.5911 - val_tn: 22738.0000 - val_tp: 1256.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.9507 - auc: 0.9447 - fn: 1451.2034 - fp: 410.4350 - loss: 0.1377 - prc: 0.7392 - precision: 0.8066 - recall: 0.5466 - tn: 33966.6914 - tp: 1707.6511 - val_accuracy: 0.9593 - val_auc: 0.9570 - val_fn: 825.0000 - val_fp: 193.0000 - val_loss: 0.1172 - val_prc: 0.8082 - val_precision: 0.8707 - val_recall: 0.6118 - val_tn: 22682.0000 - val_tp: 1300.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9484 - auc: 0.9419 - fn: 1476.7164 - fp: 424.6426 - loss: 0.1411 - prc: 0.7363 - precision: 0.7957 - recall: 0.5363 - tn: 33906.4570 - tp: 1728.1625 - val_accuracy: 0.9600 - val_auc: 0.9583 - val_fn: 863.0000 - val_fp: 137.0000 - val_loss: 0.1174 - val_prc: 0.8085 - val_precision: 0.9021 - val_recall: 0.5939 - val_tn: 22738.0000 - val_tp: 1262.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9496 - auc: 0.9457 - fn: 1472.6030 - fp: 417.2055 - loss: 0.1362 - prc: 0.7459 - precision: 0.8034 - recall: 0.5349 - tn: 33925.7852 - tp: 1720.3846 - val_accuracy: 0.9596 - val_auc: 0.9584 - val_fn: 843.0000 - val_fp: 166.0000 - val_loss: 0.1171 - val_prc: 0.8083 - val_precision: 0.8854 - val_recall: 0.6033 - val_tn: 22709.0000 - val_tp: 1282.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9495 - auc: 0.9453 - fn: 1477.9258 - fp: 414.1015 - loss: 0.1391 - prc: 0.7445 - precision: 0.8077 - recall: 0.5450 - tn: 33911.7891 - tp: 1732.1650 - val_accuracy: 0.9604 - val_auc: 0.9581 - val_fn: 846.0000 - val_fp: 145.0000 - val_loss: 0.1180 - val_prc: 0.8090 - val_precision: 0.8982 - val_recall: 0.6019 - val_tn: 22730.0000 - val_tp: 1279.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9502 - auc: 0.9443 - fn: 1455.2610 - fp: 408.0652 - loss: 0.1380 - prc: 0.7425 - precision: 0.8061 - recall: 0.5447 - tn: 33938.8359 - tp: 1733.8158 - val_accuracy: 0.9590 - val_auc: 0.9589 - val_fn: 906.0000 - val_fp: 119.0000 - val_loss: 0.1191 - val_prc: 0.8076 - val_precision: 0.9111 - val_recall: 0.5736 - val_tn: 22756.0000 - val_tp: 1219.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9488 - auc: 0.9411 - fn: 1516.1685 - fp: 422.0597 - loss: 0.1404 - prc: 0.7265 - precision: 0.7972 - recall: 0.5194 - tn: 33935.1133 - tp: 1662.6388 - val_accuracy: 0.9588 - val_auc: 0.9583 - val_fn: 915.0000 - val_fp: 114.0000 - val_loss: 0.1186 - val_prc: 0.8076 - val_precision: 0.9139 - val_recall: 0.5694 - val_tn: 22761.0000 - val_tp: 1210.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9502 - auc: 0.9425 - fn: 1467.0094 - fp: 415.2686 - loss: 0.1384 - prc: 0.7398 - precision: 0.8077 - recall: 0.5401 - tn: 33933.5078 - tp: 1720.1945 - val_accuracy: 0.9596 - val_auc: 0.9578 - val_fn: 867.0000 - val_fp: 142.0000 - val_loss: 0.1175 - val_prc: 0.8075 - val_precision: 0.8986 - val_recall: 0.5920 - val_tn: 22733.0000 - val_tp: 1258.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.9514 - auc: 0.9458 - fn: 1429.4734 - fp: 393.2281 - loss: 0.1363 - prc: 0.7500 - precision: 0.8192 - recall: 0.5530 - tn: 33946.8359 - tp: 1766.4418 - val_accuracy: 0.9602 - val_auc: 0.9580 - val_fn: 859.0000 - val_fp: 135.0000 - val_loss: 0.1177 - val_prc: 0.8073 - val_precision: 0.9036 - val_recall: 0.5958 - val_tn: 22740.0000 - val_tp: 1266.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9512 - auc: 0.9463 - fn: 1438.2904 - fp: 411.0699 - loss: 0.1353 - prc: 0.7520 - precision: 0.8139 - recall: 0.5531 - tn: 33926.6250 - tp: 1759.9949 - val_accuracy: 0.9593 - val_auc: 0.9583 - val_fn: 912.0000 - val_fp: 105.0000 - val_loss: 0.1182 - val_prc: 0.8089 - val_precision: 0.9203 - val_recall: 0.5708 - val_tn: 22770.0000 - val_tp: 1213.0000\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    }
   ],
   "source": [
    "model_dnn_baseline = build_model()\n",
    "\n",
    "model_dnn_baseline.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=relevant_classification_metrics\n",
    ")\n",
    "\n",
    "history_baseline = model_dnn_baseline.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs_max,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b2421-fc31-482f-84b2-576323635236",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Now we make predictions using the DNN-based model to evaluate its performance on the validation data. Comparing the results, even when changing the confidence threshold, we observed a slightly worse performance from the DNN-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62756eb1-bcb5-4ceb-9bb1-78dd9d8dfa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     22875\n",
      "           1       0.90      0.60      0.72      2125\n",
      "\n",
      "    accuracy                           0.96     25000\n",
      "   macro avg       0.93      0.80      0.85     25000\n",
      "weighted avg       0.96      0.96      0.96     25000\n",
      "\n",
      "[[22730   145]\n",
      " [  846  1279]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_baseline.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "36bae2bc-cd2d-4c6f-b4f2-e26f3f6ab068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     22875\n",
      "           1       0.73      0.70      0.72      2125\n",
      "\n",
      "    accuracy                           0.95     25000\n",
      "   macro avg       0.85      0.84      0.85     25000\n",
      "weighted avg       0.95      0.95      0.95     25000\n",
      "\n",
      "[[22339   536]\n",
      " [  642  1483]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_baseline.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.3).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df14e9-a4d7-4fb1-9c99-6444c939e634",
   "metadata": {},
   "source": [
    "\n",
    "Therefore, if we had a larger dataset, the DNN approach would likely outperform the tree-based model due to its ability to learn more complex patterns. Given this characteristic, we will not repeat the undersampling test previously applied on the Tree-based approach. However, we will test an alternative version of the model trained with different class weights, as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49a2dc-465a-4a10-8d37-ebf6090e8ea5",
   "metadata": {},
   "source": [
    "### Alternative DNN Model (Using class weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "deb6787e-2150-4de4-98ca-a3f897071d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11ms/step - accuracy: 0.9124 - auc: 0.9159 - fn: 1776.8179 - fp: 3845.9727 - loss: 0.5217 - prc: 0.6128 - precision: 0.4946 - recall: 0.6536 - tn: 53381.1641 - tp: 3532.0251 - val_accuracy: 0.9134 - val_auc: 0.9590 - val_fn: 365.0000 - val_fp: 1799.0000 - val_loss: 0.1982 - val_prc: 0.8055 - val_precision: 0.4945 - val_recall: 0.8282 - val_tn: 21076.0000 - val_tp: 1760.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9116 - auc: 0.9389 - fn: 771.1416 - fp: 2545.8674 - loss: 0.3859 - prc: 0.6901 - precision: 0.4827 - recall: 0.7565 - tn: 31805.8750 - tp: 2413.0947 - val_accuracy: 0.9251 - val_auc: 0.9593 - val_fn: 412.0000 - val_fp: 1460.0000 - val_loss: 0.1772 - val_prc: 0.8074 - val_precision: 0.5399 - val_recall: 0.8061 - val_tn: 21415.0000 - val_tp: 1713.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9149 - auc: 0.9425 - fn: 744.4200 - fp: 2424.5164 - loss: 0.3761 - prc: 0.7009 - precision: 0.4978 - recall: 0.7610 - tn: 31948.6836 - tp: 2418.3594 - val_accuracy: 0.9295 - val_auc: 0.9590 - val_fn: 447.0000 - val_fp: 1316.0000 - val_loss: 0.1731 - val_prc: 0.8074 - val_precision: 0.5605 - val_recall: 0.7896 - val_tn: 21559.0000 - val_tp: 1678.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.9179 - auc: 0.9464 - fn: 722.3365 - fp: 2378.3459 - loss: 0.3613 - prc: 0.7276 - precision: 0.5083 - recall: 0.7729 - tn: 31978.4863 - tp: 2456.8103 - val_accuracy: 0.9311 - val_auc: 0.9593 - val_fn: 458.0000 - val_fp: 1265.0000 - val_loss: 0.1683 - val_prc: 0.8076 - val_precision: 0.5686 - val_recall: 0.7845 - val_tn: 21610.0000 - val_tp: 1667.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.9198 - auc: 0.9485 - fn: 704.4691 - fp: 2322.2080 - loss: 0.3543 - prc: 0.7324 - precision: 0.5157 - recall: 0.7810 - tn: 32051.3086 - tp: 2457.9944 - val_accuracy: 0.9284 - val_auc: 0.9592 - val_fn: 445.0000 - val_fp: 1345.0000 - val_loss: 0.1761 - val_prc: 0.8079 - val_precision: 0.5554 - val_recall: 0.7906 - val_tn: 21530.0000 - val_tp: 1680.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.9183 - auc: 0.9462 - fn: 712.8038 - fp: 2327.7163 - loss: 0.3622 - prc: 0.7303 - precision: 0.5117 - recall: 0.7730 - tn: 32047.3340 - tp: 2448.1245 - val_accuracy: 0.9355 - val_auc: 0.9592 - val_fn: 480.0000 - val_fp: 1133.0000 - val_loss: 0.1623 - val_prc: 0.8067 - val_precision: 0.5922 - val_recall: 0.7741 - val_tn: 21742.0000 - val_tp: 1645.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.9164 - auc: 0.9466 - fn: 710.6555 - fp: 2407.2832 - loss: 0.3650 - prc: 0.7325 - precision: 0.5102 - recall: 0.7788 - tn: 31907.7188 - tp: 2510.3220 - val_accuracy: 0.9292 - val_auc: 0.9591 - val_fn: 441.0000 - val_fp: 1329.0000 - val_loss: 0.1735 - val_prc: 0.8070 - val_precision: 0.5589 - val_recall: 0.7925 - val_tn: 21546.0000 - val_tp: 1684.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - accuracy: 0.9192 - auc: 0.9502 - fn: 702.4282 - fp: 2345.5500 - loss: 0.3513 - prc: 0.7400 - precision: 0.5171 - recall: 0.7831 - tn: 31993.8242 - tp: 2494.1760 - val_accuracy: 0.9225 - val_auc: 0.9591 - val_fn: 410.0000 - val_fp: 1528.0000 - val_loss: 0.1888 - val_prc: 0.8068 - val_precision: 0.5288 - val_recall: 0.8071 - val_tn: 21347.0000 - val_tp: 1715.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9177 - auc: 0.9477 - fn: 707.3539 - fp: 2387.4094 - loss: 0.3562 - prc: 0.7251 - precision: 0.5041 - recall: 0.7801 - tn: 31969.3887 - tp: 2471.8276 - val_accuracy: 0.9300 - val_auc: 0.9592 - val_fn: 453.0000 - val_fp: 1298.0000 - val_loss: 0.1680 - val_prc: 0.8075 - val_precision: 0.5630 - val_recall: 0.7868 - val_tn: 21577.0000 - val_tp: 1672.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.9181 - auc: 0.9494 - fn: 712.8034 - fp: 2368.8179 - loss: 0.3595 - prc: 0.7439 - precision: 0.5235 - recall: 0.7846 - tn: 31927.0840 - tp: 2527.2737 - val_accuracy: 0.9339 - val_auc: 0.9593 - val_fn: 467.0000 - val_fp: 1186.0000 - val_loss: 0.1727 - val_prc: 0.8078 - val_precision: 0.5830 - val_recall: 0.7802 - val_tn: 21689.0000 - val_tp: 1658.0000\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n"
     ]
    }
   ],
   "source": [
    "model_dnn_alternative_weights = build_model()\n",
    "\n",
    "model_dnn_alternative_weights.compile(\n",
    "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=relevant_classification_metrics\n",
    ")\n",
    "\n",
    "history_alternative_weights = model_dnn_alternative_weights.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs_max,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(X_test, y_test),\n",
    "    class_weight={0: 1.0, 1: 5.0} # Applies a 5x higher weight to diabetic samples.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226bfd4-2b53-43f5-8072-ea58c243f1c5",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "As we saw before, the class weights in fact reduced the false negatives with the trade-off of incresing the false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e42ccb8-1c53-47b9-97f0-526ed80bab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     22875\n",
      "           1       0.56      0.79      0.65      2125\n",
      "\n",
      "    accuracy                           0.93     25000\n",
      "   macro avg       0.77      0.87      0.81     25000\n",
      "weighted avg       0.94      0.93      0.93     25000\n",
      "\n",
      "[[21530  1345]\n",
      " [  445  1680]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_alternative_weights.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bad5c171-57b6-4093-9d65-75dabd4a30ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93     22875\n",
      "           1       0.39      0.89      0.54      2125\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.69      0.88      0.73     25000\n",
      "weighted avg       0.94      0.87      0.89     25000\n",
      "\n",
      "[[19930  2945]\n",
      " [  234  1891]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = model_dnn_alternative_weights.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.3).astype(\"int32\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170bd023-0c26-47b0-9c4b-52e05d0e786a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
